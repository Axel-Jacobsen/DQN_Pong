{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Pong\n",
    "\n",
    "Here is our final implementation of DQN Pong. This will include the model, replay memory, and training code. Highlights are the state preprocessing and the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Here is the model and replay memory. Notable sections of code is the `DQN` class. It shows our GRU and feed forward layer configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    # Returns list of Transitions\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def count(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    '''DQN'''\n",
    "\n",
    "    def __init__(self, in_features=4, n_actions=3, device='cpu'):\n",
    "        self.device = device\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "        self.gru = nn.GRU(in_features, 256, 3)\n",
    "        self.linear1 = nn.Linear(256, 512)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,_ = self.gru(x.float())\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.leaky_relu(self.linear1(x))\n",
    "        x = F.leaky_relu(self.linear2(x))\n",
    "        return self.linear3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNCOMMENT IF YOU WANT TO TRAIN'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! /usr/bin/env python3\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from scipy.ndimage.measurements import center_of_mass\n",
    "import random\n",
    "\n",
    "from model import ReplayMemory, DQN, Transition\n",
    "\n",
    "class TrainPongV0(object):\n",
    "    \"\"\"\n",
    "    Class for training a DQN model for the Pong V0 Network\n",
    "    \"\"\"\n",
    "\n",
    "    # hyperparameters\n",
    "    BATCH_SIZE = 64\n",
    "    GAMMA = 0.99\n",
    "    EPSILON_START = 1\n",
    "    EPSILON_FINAL = 0.05\n",
    "    EPSILON_DECAY = 10000000\n",
    "    TARGET_UPDATE = 100\n",
    "    lr = 1e-5\n",
    "    INITIAL_MEMORY = 10000\n",
    "    MEMORY_SIZE = 5 * INITIAL_MEMORY\n",
    "\n",
    "    def __init__(self, target: DQN, policy: DQN, memory: ReplayMemory, device):\n",
    "        self.target = target\n",
    "        self.policy = policy\n",
    "        self.memory = memory\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.lr)\n",
    "        self.steps = 0\n",
    "        self.episodes = 0\n",
    "        self.device = device\n",
    "        self.total_rewards = [] # Total rewards - dt is 1 episode\n",
    "\n",
    "    @property\n",
    "    def epsilon(self):\n",
    "        return (self.EPSILON_FINAL + (self.EPSILON_START - self.EPSILON_FINAL) * np.exp(-1 * self.steps / self.EPSILON_DECAY))\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_state(s: np.ndarray, prev_s=None):\n",
    "        \"\"\"Returns the simplified state for the last 5 timesteps.\n",
    "\n",
    "        Shape of the simplified state is (time_seq, batch, input_size)\n",
    "\n",
    "        Args:\n",
    "            s (ndarray) - Game state returned by Gym for the current time step\n",
    "            prev_s (ndarray) - Previous state in the format returned by this\n",
    "                function. Shape is (time_seq, batch, input_size)\n",
    "        \"\"\"\n",
    "        if prev_s is None:\n",
    "            prev_s = np.zeros((10,1,4))\n",
    "\n",
    "\n",
    "        # Get rid of useless rows and the green & blue colour chanels\n",
    "        reduced_rows = s[34:194, :, 0]\n",
    "\n",
    "        # Background is 0, paddles & ball is 1 (R value of backround 144)\n",
    "        masked = (reduced_rows != 144).astype(int)\n",
    "\n",
    "        # Center of our paddle (dqn paddle), opponent paddle, and ball y and x coordinates\n",
    "        dqn_y, _ = center_of_mass(masked[:, 140:144])\n",
    "        opp_y, _ = center_of_mass(masked[:, 16:20])\n",
    "        ball_y, ball_x = center_of_mass(masked[:, 20:140])\n",
    "\n",
    "        dqn_y = 80 if np.isnan(dqn_y) else dqn_y\n",
    "        opp_y = 80 if np.isnan(opp_y) else opp_y\n",
    "        # x position of ball is offset by 21 px to the center of img\n",
    "        ball_x = 80 if np.isnan(ball_x) else ball_x + 21\n",
    "        ball_y = 80 if np.isnan(ball_y) else ball_y\n",
    "\n",
    "        state_vec = np.array([[[opp_y, dqn_y, ball_x, ball_y]]])\n",
    "        state_vec = np.concatenate((state_vec, prev_s))[:10]\n",
    "\n",
    "        return state_vec\n",
    "\n",
    "\n",
    "    def select_action(self, state, env):\n",
    "        \"\"\"Select an action using randomized greedy.\n",
    "\n",
    "        action space: integers [0,1,2,3,4,5] - represents movements of \n",
    "        [do nothing, do nothing, up, down, up, down]\n",
    "\n",
    "        In our networks action space it will output a number between 0 and 2.\n",
    "        0 is NOP, 1 is up, 2 is down. Adding 1 to this will give correct mapping to\n",
    "        actions as defined by the environment\n",
    "\n",
    "        If a random number is below epsilon, sample a random action from the action \n",
    "        space. Otherwise, choose the best action according to the current policy.\n",
    "\n",
    "        Args:\n",
    "            state (ndarray) - Array of shape (time_seq, batch, input_size) consisting of \n",
    "                paddle and ball positions for the last 5 frames.\n",
    "            env - Gym environment\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return torch.tensor(random.choice([0,1,2]), device=self.device)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.from_numpy(state)\n",
    "                res = self.policy(state.to(self.device))\n",
    "                return res[0].argmax()  # Max from the most recent time step\n",
    "\n",
    "\n",
    "    def memory_replay(self):\n",
    "        \"\"\"\n",
    "        This method was more or less copied from https://github.com/jmichaux/dqn-pytorch/blob/master/main.py#L38 - It is a very clean solution, very readable. \n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # Returns list of Transitions\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        actions = tuple(\n",
    "                (map(lambda a: torch.tensor([[a]], device=self.device), batch.action)))\n",
    "\n",
    "        rewards = tuple(\n",
    "                (map(lambda r: torch.tensor([r], device=self.device), batch.reward)))\n",
    "\n",
    "        non_final_mask = torch.tensor(\n",
    "                tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                device=self.device)\n",
    "\n",
    "        non_final_next_states = torch.tensor([s for s in batch.next_state\n",
    "            if s is not None]).to(self.device)\n",
    "        non_final_next_states = non_final_next_states.squeeze().transpose(0,1)\n",
    "\n",
    "        unwrapped_states = np.array(batch.state).squeeze()\n",
    "        # Reshape from (batch, time, input) to (time, batch, input)\n",
    "        unwrapped_states = np.transpose(unwrapped_states, (1,0,2))\n",
    "\n",
    "        state_batch = torch.tensor(unwrapped_states).to(self.device)\n",
    "        action_batch = torch.tensor(actions).to(self.device)\n",
    "        reward_batch = torch.tensor(rewards).to(self.device)\n",
    "\n",
    "        # Value of current state as predicted by policy network\n",
    "        state_action_values = self.policy(state_batch)[0]  # index 0 gets most recent timestep\n",
    "        state_action_values = state_action_values.gather(1,action_batch.reshape((-1,1)))\n",
    "\n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target(\n",
    "                non_final_next_states)[0].max(1)[0].detach()\n",
    "\n",
    "        expected_state_action_values = (\n",
    "                next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        a = list(self.policy.parameters())[0].clone()\n",
    "        loss.backward()\n",
    "        for param in self.policy.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        b = list(self.policy.parameters())[0].clone()\n",
    "        assert not torch.equal(a.data,b.data)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_memory(path):\n",
    "        return (np.load(path, allow_pickle=True)).item()\n",
    "\n",
    "    def train(self, num_episodes: int):\n",
    "        env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "        batch_reward = 0.0\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            self.episodes += 1\n",
    "            state = env.reset()\n",
    "            state = self.prepare_state(state)\n",
    "            tot_reward = 0\n",
    "\n",
    "            while True:\n",
    "                action = self.select_action(state, env)\n",
    "                obs, reward, done, _ = env.step(action+1)  # Actions are range [0,2] but env expects [1,3]\n",
    "                self.steps += 1\n",
    "\n",
    "                if not done:\n",
    "                    next_state = self.prepare_state(obs, prev_s=state)\n",
    "                else:\n",
    "                    next_state = None\n",
    "\n",
    "                tot_reward += reward\n",
    "                batch_reward += reward\n",
    "                reward = torch.tensor([reward], device=self.device)\n",
    "\n",
    "                self.memory.push(state, action.to(self.device),\n",
    "                        reward.to(self.device), next_state, done)\n",
    "                state = next_state\n",
    "\n",
    "                if self.steps > self.INITIAL_MEMORY or len(self.memory) >= self.INITIAL_MEMORY:\n",
    "                    self.memory_replay()\n",
    "                    if self.steps % self.TARGET_UPDATE == 0:\n",
    "                        self.target.load_state_dict(policy.state_dict())\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.total_rewards.append(tot_reward)\n",
    "\n",
    "            if (self.episodes) % 20 == 0:\n",
    "                print('\\rTotal steps: {} \\t Episode: {}/{} \\t Batch reward: {:.3f} \\t Last reward: {:.3f} \\t Epsilon: {:.3f}'.format(\n",
    "                    self.steps, episode+1, num_episodes, batch_reward, tot_reward, self.epsilon))\n",
    "\n",
    "                batch_reward = 0\n",
    "\n",
    "                if (self.episodes) % 100 == 0:\n",
    "                    policy_PATH = f'policies/policy_episode_{self.episodes}_{self.steps}'\n",
    "                    target_PATH = f'targets/target_episode_{self.episodes}_{self.steps}'\n",
    "                    torch.save(self.policy.state_dict(), policy_PATH)\n",
    "                    torch.save(self.target.state_dict(), target_PATH)\n",
    "\n",
    "        policy_PATH = f'policy_episode_{self.episodes}_{self.steps}'\n",
    "        target_PATH = f'target_episode_{self.episodes}_{self.steps}'\n",
    "        torch.save(self.policy.state_dict(), policy_PATH)\n",
    "        torch.save(self.target.state_dict(), target_PATH)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "\n",
    "\"\"\"UNCOMMENT IF YOU WANT TO TRAIN\"\"\"\n",
    "# if __name__ == '__main__':\n",
    "#     device = torch.device(\n",
    "#             \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#             )\n",
    "\n",
    "#     print(f'Using Device {device}')\n",
    "\n",
    "#     target = DQN(device=device).to(device)\n",
    "#     policy = DQN(device=device).to(device)\n",
    "#     mem = ReplayMemory(TrainPongV0.MEMORY_SIZE)\n",
    "#     trainer = TrainPongV0(target, policy, mem, device)\n",
    "\n",
    "#     try:\n",
    "#         trainer.train(50000)\n",
    "#     finally:\n",
    "#         np.save('rewards', trainer.total_rewards, allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/scipy/ndimage/measurements.py:1359: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  for dir in range(input.ndim)]\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from model import DQN\n",
    "from train import TrainPongV0\n",
    "\n",
    "def render_model(path, for_gif=False, epsilon=None):\n",
    "    env = gym.make('PongDeterministic-v4')\n",
    "    dqn = DQN()\n",
    "    dqn.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "    dqn.eval()\n",
    "\n",
    "    obs = env.reset()\n",
    "    s = TrainPongV0.prepare_state(obs)\n",
    "    epsilon = 0.15\n",
    "    try:\n",
    "        for i in range(15000):\n",
    "\n",
    "            env.render()\n",
    "\n",
    "            if np.random.rand() < epsilon:\n",
    "                a = np.random.choice([1,2,3])\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    a = dqn(torch.from_numpy(s))[0].argmax() + 1\n",
    "\n",
    "            prev_s = s\n",
    "            obs, r, d, _ = env.step(a)\n",
    "            s = TrainPongV0.prepare_state(obs, prev_s=prev_s)\n",
    "            if d:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    env.close()\n",
    "    return None, None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    render_model('policy_episode_12900_5687803')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
